import os
from pathlib import Path
from tech_analysis.data.fetcher import fetch_stock_data, clean_and_validate_data
from tech_analysis.backtest import portfolio_backtest, calculate_performance_metrics, correlate_performance_with_regimes
from report_generator import generate_markdown_report
import json
from tech_analysis.market_regimes import detect_market_regime_series

def run_pipeline(tickers, output_dir=None):
    """
    Orchestrates fetching, backtesting, and reporting for a unified portfolio of tickers.
    Minimal implementation for integration test.
    """
    # Set up output directory
    if output_dir:
        reports_dir = Path(output_dir) / "reports"
        reports_dir.mkdir(parents=True, exist_ok=True)
        os.chdir(output_dir)
    # Load config
    config_path = Path(__file__).parent / "config.json"
    if config_path.exists():
        with open(config_path, "r") as f:
            config = json.load(f)
        period = config.get("period", "10y")
        initial_cash = config.get("initial_cash", 10000)
        position_size = config.get("position_size", 100)
        strategy = config.get("strategy", "naive_momentum")
        strategy_params = config.get("strategy_params", {})
    else:
        period = "10y"
        initial_cash = 10000
        position_size = 100
        strategy = "naive_momentum"
        strategy_params = {}
    # Filter out invalid tickers
    invalid_tickers = {'', '.', None}
    filtered_tickers = [t for t in tickers if t not in invalid_tickers]
    if not filtered_tickers:
        raise ValueError("No valid tickers provided after filtering invalid ones.")
    # Fetch and clean data for all filtered tickers
    data_dict = {}
    for ticker in filtered_tickers:
        df = fetch_stock_data(ticker, period=period)
        df = clean_and_validate_data(df)
        df.columns = df.columns.str.lower()
        # DEBUG: Data quality
        # print(f"[DEBUG] {ticker} data length: {len(df)}; head: {df.head(2)}; tail: {df.tail(2)}")
        if df is not None and not df.empty and 'close' in df.columns:
            data_dict[ticker] = df
        else:
            print(f"[WARN] Skipping {ticker}: no valid data.")
    if not data_dict:
        raise ValueError("No valid data for any ticker. Cannot generate unified report.")
    # After fetching and cleaning data for all tickers, aggregate close prices for regime detection
    # For simplicity, use the first ticker's close prices (or aggregate as needed for your use case)
    sample_ticker = next(iter(data_dict))
    close_prices = data_dict[sample_ticker]['close']
    # Compute full-date-range regime series
    regime_series = detect_market_regime_series(close_prices)
    # Unified portfolio backtest
    bt_result = portfolio_backtest(data_dict, initial_cash=initial_cash, position_size=position_size, strategy_params=strategy_params)
    # DEBUG: Backtest result
    print("[DEBUG] portfolio_backtest result:", bt_result)
    strategy_params = bt_result.get('strategy_params', {})
    pf = bt_result['portfolio_state']
    trade_log = bt_result['trade_log']
    # DEBUG: Trade log with rationale
    print("[DEBUG] Trade log:", trade_log)
    # Extract the real equity curve from the portfolio state
    equity_curve = pf.equity_curve if hasattr(pf, 'equity_curve') else None
    if equity_curve is None or len(equity_curve) == 0:
        raise ValueError("No equity curve generated by portfolio backtest.")
    # Compute real stats using actual equity curve and trade log
    stats = calculate_performance_metrics(equity_curve, trade_log)
    stats['_trades'] = trade_log
    stats['trades'] = trade_log  # Ensure compatibility with report generator
    stats['equity_curve'] = equity_curve
    stats['regime_series'] = regime_series
    # Compute regime summary string from trade log
    regime_stats = correlate_performance_with_regimes(trade_log)
    if regime_stats and any(regime_stats.values()):
        total = sum(v['count'] for v in regime_stats.values())
        summary_parts = []
        for regime, v in regime_stats.items():
            regime_str = regime.capitalize() if isinstance(regime, str) and regime else "Unknown"
            percent = 100 * v['count'] / total if total else 0
            summary_parts.append(f"{regime_str}: {percent:.0f}%")
        regime_summary = ', '.join(summary_parts)
    else:
        regime_summary = 'No trades or regimes detected.'
    stats['regime_summary'] = regime_summary

    # --- UPDATE: Per-date regime summary table using regime_series ---
    regime_lines = ["| Date | Regime |", "|------|--------|"]
    if 'regime_series' in stats and hasattr(stats['regime_series'], 'items'):
        for date, regime in stats['regime_series'].items():
            regime_lines.append(f"| {date.strftime('%Y-%m-%d')} | {regime} |")
    if len(regime_lines) > 2:
        stats['regime_summary'] += "\n\n" + "\n".join(regime_lines)
    # --- END UPDATE ---

    stats['strategy_params'] = strategy_params
    # DEBUG: Metrics/stats for report
    print("[DEBUG] Stats for report:", stats)
    # Pass real stats to report generator
    try:
        generate_markdown_report(stats, pf)
    except Exception as e:
        import traceback
        print(f"[DEBUG] Exception in generate_markdown_report: {e}")
        traceback.print_exc()
        raise

if __name__ == "__main__":
    import traceback
    from tech_analysis.data.stocks_list import STOCKS_LIST
    output_dir = "."
    print(f"[INFO] Running unified pipeline for all tickers in STOCKS_LIST...")
    try:
        print("[DEBUG] About to call run_pipeline")
        run_pipeline(STOCKS_LIST, output_dir)
        print(f"[INFO] Unified portfolio report generated.")
    except Exception as e:
        print(f"[ERROR] Exception in main pipeline: {e}")
        traceback.print_exc()
    print("[DEBUG] End of main block")
    print(f"[INFO] Pipeline completed. Unified PDF report should be generated in {output_dir}.")
